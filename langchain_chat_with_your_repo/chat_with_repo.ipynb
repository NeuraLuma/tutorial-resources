{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chat with your Repo - Langchain Example\n",
    "With this example notebook, you will learn to utilize Langchain to add Repo Context to your Queries to ask detailed questions.\n",
    "This example will use OpenAI GPT 3.5 and the Vector Store Chroma. However, due to the modularity of Langchain you can easily swap the LLM and Vectorstore easily to adapt to your preferences."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "First let us beginn by installing necessary dependencies (if not already installed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install openai langchain ipywidgets chroma GitPython chromadb tiktoken"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then let us import necessary packages and load the env for our OpenAI Key:\n",
    "\n",
    "Note: Make sure you have created a .env file with OPENAI_API_KEY=sk-..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from functools import partial\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file for our API Keys\n",
    "from langchain.chat_models import ChatOpenAI # Switch to another provider as you please\n",
    "from langchain.document_loaders import GitLoader # Used to load the repo\n",
    "from langchain.text_splitter import (\n",
    "    RecursiveCharacterTextSplitter,\n",
    "    Language,\n",
    ") # Text splitter for our codebase\n",
    "from langchain.embeddings import OpenAIEmbeddings # Embeddings for the LLM injection\n",
    "from langchain.vectorstores import Chroma # Our Vectorstore\n",
    "from langchain.chains import RetrievalQA # Chain to chat with our documents\n",
    "from langchain.memory import ConversationTokenBufferMemory # Memory for our conversation\n",
    "import ipywidgets as widgets # Widgets for our chat\n",
    "from IPython.display import display, Markdown # Used to display the chat\n",
    "from langchain.prompts import ChatPromptTemplate"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters\n",
    "Now let us define our Parameters necessary for running our notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_url = \"https://github.com/ganlanyuan/tiny-slider\" # Replace with your repo\n",
    "local_path = os.getcwd() + '/repo' # Replace with your local path\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0) # Replace with your Chat-optimized LLM"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets define a helper function to retrieve our Loader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loader(local_path, repo_url, branch = 'master', has_file_ext = ['.md', '.js', '.html'], ignore_paths = ['dist/']):\n",
    "    \"\"\"\n",
    "    Helper function to create a Loader to load the repo\n",
    "\n",
    "    Args:\n",
    "        local_path (str): Path to the local repo\n",
    "        repo_url (str): URL of the repo to clone from\n",
    "        branch (str): Branch of the repo to checkout\n",
    "        has_file_ext (list): ist of file extensions to load\n",
    "        ignore_paths (list): List of paths to ignore\n",
    "    \n",
    "    Returns:\n",
    "        GitLoader Object\n",
    "    \"\"\"\n",
    "    \n",
    "    # If the path exists, the GitLoader will throw an Error when trying to clone\n",
    "    if os.path.exists(local_path):\n",
    "        repo_url = None\n",
    "    \n",
    "    file_filter_functions = []\n",
    "\n",
    "    def not_in_ignore_paths(file_path, ignore_paths):\n",
    "        return all(file_path.find(path) == -1 for path in ignore_paths)\n",
    "\n",
    "    def has_allowed_extension(file_path, extensions):\n",
    "        return any(file_path.endswith(ext) for ext in extensions)\n",
    "\n",
    "    if len(ignore_paths):\n",
    "        file_filter_functions.append(partial(not_in_ignore_paths, ignore_paths=ignore_paths))\n",
    "\n",
    "    if len(has_file_ext):\n",
    "        file_filter_functions.append(partial(has_allowed_extension, extensions=has_file_ext))\n",
    "\n",
    "    def file_filter_function(file_path):\n",
    "        return all(func(file_path) for func in file_filter_functions)\n",
    "    \n",
    "    return GitLoader(repo_path=local_path, clone_url=repo_url, branch=branch, file_filter=file_filter_function)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our implementation, we use the RecursiveCharacterTextSplitter to split the documents. Let us define a helper function to create a list of documents in the Languages of the Repo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_docs(docs):\n",
    "    \"\"\"\n",
    "    Helper function to split the docs into chunks by supported languages.\n",
    "\n",
    "    Args:\n",
    "        docs (list): List of documents to split\n",
    "    \n",
    "    Returns:\n",
    "        list of documents\n",
    "    \"\"\"\n",
    "    \n",
    "    js_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "        language=Language.JS, chunk_size=1024, chunk_overlap=0\n",
    "    )\n",
    "    html_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "        language=Language.HTML, chunk_size=1024, chunk_overlap=0\n",
    "    )\n",
    "    markdown_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "        language=Language.MARKDOWN, chunk_size=1024, chunk_overlap=0\n",
    "    )\n",
    "\n",
    "    # Only retrieve the text from the documents\n",
    "    text_docs = [doc.page_content for doc in docs]\n",
    "    \n",
    "    js, html, markdown = js_splitter.create_documents(texts=text_docs), html_splitter.create_documents(texts=text_docs), markdown_splitter.create_documents(texts=text_docs)\n",
    "\n",
    "    # merge the docs to List\n",
    "    return js + html + markdown"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us retrieve our loader and the split docs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = get_loader(local_path, repo_url)\n",
    "docs = loader.load()\n",
    "split_docs = split_docs(docs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can convert them to Embeddings that we can then store to our vector store\n",
    "\n",
    "(Note: each time we run this cell we will infer the API, for production you should store the embeddings locally or on a server so you don't always pay API costs):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstore = Chroma.from_documents(documents=split_docs, embedding=embeddings)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's define the retriever:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_retriever = vectorstore.as_retriever(search_kwargs={\"k\": 4}) # You can adjust search_kwargs here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us define our memory to keep some of the conversation for context:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationTokenBufferMemory(llm=llm, max_token_limit=512) # Adjust max_token_limit depending how much information you want to keep from the conversation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us define a prompt template for use when calling our chain. This one is a modified version from the original one to give a bit better context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    Your task is to assist with questions from a code repository. \\\n",
    "    The Repository is a library called tiny slider. \\\n",
    "    Use the following pieces of context to answer the question at the end. \\\n",
    "    The context are snippets of files from the repository \\\n",
    "    When answering with code snippets, make sure to wrap them in the correct syntax using markdown backticks. \\\n",
    "    If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "    {context}\n",
    "\n",
    "    Question: {question}\n",
    "    Helpful Answer:\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can define the chain that we will use to infer questions regarding our Git Repository:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = RetrievalQA.from_llm(\n",
    "    llm=llm,\n",
    "    memory=memory, \n",
    "    retriever=vector_retriever, \n",
    "    prompt=prompt_template, \n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating an interactive chat interface to chat with our Repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the text input widget\n",
    "text_input = widgets.Text(\n",
    "    value='',\n",
    "    placeholder='Type something...',\n",
    "    description='Input:',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "# Create an output widget to display the conversation history\n",
    "output = widgets.Output()\n",
    "\n",
    "def format_input(user, user_input):\n",
    "    return f\"## {user}: \\n --- \\n {user_input}\"\n",
    "\n",
    "# Function to handle the text input\n",
    "def handle_submit(sender):\n",
    "    with output:\n",
    "        user_input = text_input.value\n",
    "        # Clear the input box for the next message\n",
    "        text_input.value = ''\n",
    "        # Display the user's input\n",
    "        display(Markdown(format_input('User', user_input)))\n",
    "        result = chain.run(user_input)\n",
    "        display(Markdown(format_input('AI', result)))\n",
    "\n",
    "with output:\n",
    "    display(Markdown('# Chat with your repo'))\n",
    "\n",
    "# Link the function to the text input's submission event\n",
    "text_input.on_submit(handle_submit)\n",
    "\n",
    "# Display the widgets\n",
    "display(output, text_input)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
