{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4fce3fd6",
   "metadata": {},
   "source": [
    "Install necessary dependencies (if you are using CPU):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268b7d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install ipykernel notebook transformers torch torchvision torchaudio datasets \"datasets[audio]\" \"jax[cpu]==0.4.11\" git+https://github.com/sanchit-gandhi/whisper-jax.git cached_property\n",
    "%conda install ffmpeg -c conda-forge "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5d59a4",
   "metadata": {},
   "source": [
    "Install necessary dependencies (if you are using GPU):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd5d5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install ipykernel notebook transformers torch torchvision torchaudio datasets \"datasets[audio]\" \"jax==0.4.11\" git+https://github.com/sanchit-gandhi/whisper-jax.git cached_property\n",
    "%conda install ffmpeg -c conda-forge \n",
    "%conda install cuda-nvcc -c nvidia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad5b6f6",
   "metadata": {},
   "source": [
    "## Transformer Implementation\n",
    "Now we can use the `transformers` library to load the ASR model checkpoint `whisper-large-v2`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec0c416-7397-469a-905d-354e816105d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "whisper_pipeline = pipeline(model=\"openai/whisper-large-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf84faa",
   "metadata": {},
   "source": [
    "Lets use Mozilla's Common Voice Dataset to get some samples that we can use.\n",
    "Note that we are using `streaming` here to avoid downloading the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c2fecc-740e-4fe4-a596-221e1bf0b6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "audio_dataset = load_dataset(\"mozilla-foundation/common_voice_11_0\", \"en\", split=\"test\", streaming=True)\n",
    "audio_data_samples = audio_dataset.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1205ad",
   "metadata": {},
   "source": [
    "Now we can start creating some transcriptions using the pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdca675b",
   "metadata": {},
   "outputs": [],
   "source": [
    "transcriptions = [whisper_pipeline(sample[\"audio\"]) for sample in audio_data_samples]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f92fe6e",
   "metadata": {},
   "source": [
    "Let's have a look at the resulting transcriptions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23599f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(transcriptions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602bb034",
   "metadata": {},
   "source": [
    "Great! Let's build a simple Huggingface Dataset that contains our Transcriptions and the source Audio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb495d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, Audio\n",
    "\n",
    "new_dataset = Dataset.from_dict({\n",
    "    \"audio\": [(sample[\"audio\"]) for sample in audio_data_samples],\n",
    "    \"transcription\": [transcription[\"text\"] for transcription in transcriptions]\n",
    "}).cast_column(\"audio\", Audio())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8386cc",
   "metadata": {},
   "source": [
    "Let's inspect the dataset and their features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084b37f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(new_dataset)\n",
    "print(new_dataset.features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b0a262",
   "metadata": {},
   "source": [
    "Now we can push our dataset to the hub to the split `example`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65382155",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset.push_to_hub(\"myuser/testset\", split=\"example\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c756e1",
   "metadata": {},
   "source": [
    "## Using Jax Whisper\n",
    "Now that we have used the original Transformers implementation, let us test the implementation used in Whisper JAX by https://github.com/sanchit-gandhi.\n",
    "Let's start by defining a pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb56331",
   "metadata": {},
   "outputs": [],
   "source": [
    "from whisper_jax import FlaxWhisperPipline\n",
    "import jax.numpy as jnp\n",
    "\n",
    "# Most users should use jnp.float16, use jnp.bfloat16 for A100 / TPU\n",
    "jax_pipeline = FlaxWhisperPipline(\"openai/whisper-large-v2\", dtype=jnp.float16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18545361",
   "metadata": {},
   "source": [
    "Let's load a dataset and create transcriptions using Jax Whisper:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e515c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "audio_dataset = load_dataset(\"mozilla-foundation/common_voice_11_0\", \"en\", split=\"test\", streaming=True)\n",
    "audio_data_samples = audio_dataset.take(10)\n",
    "transcriptions = [jax_pipeline(sample[\"audio\"], task=\"transcribe\") for sample in audio_data_samples]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef6abeb",
   "metadata": {},
   "source": [
    "Let's take a look at the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630c52d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(transcriptions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705c6059",
   "metadata": {},
   "source": [
    "We can also activate timestamps easily:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3493101",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamped_transcriptions = [jax_pipeline(sample[\"audio\"], task=\"transcribe\", return_timestamps=True) for sample in audio_data_samples]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b87dca8",
   "metadata": {},
   "source": [
    "Let's have another look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea679ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(timestamped_transcriptions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcafd3ff",
   "metadata": {},
   "source": [
    "Let's also create a dataset and push it to hub:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a674a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, Audio\n",
    "\n",
    "jax_dataset = Dataset.from_dict({\n",
    "    \"audio\": [(sample[\"audio\"]) for sample in audio_data_samples],\n",
    "    \"transcription\": [transcription[\"text\"] for transcription in timestamped_transcriptions],\n",
    "    \"chunks\": [transcription[\"chunks\"] for transcription in timestamped_transcriptions]\n",
    "}).cast_column(\"audio\", Audio())\n",
    "\n",
    "jax_dataset.push_to_hub(\"myuser/testsetjax\", split=\"example\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
